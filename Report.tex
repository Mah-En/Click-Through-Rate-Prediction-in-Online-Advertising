% ------------------------------------------------------------------------------
% Project 1 – Click-Through-Rate Prediction: Complete Report  (≥14 pages)
% Machine Learning Fundamentals – 4th Assignment
% Shahid Beheshti University — June 2024
% ------------------------------------------------------------------------------
\documentclass[12pt,a4paper]{article}

% --------------------------- packages -----------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
  \geometry{margin=1in}
\usepackage{setspace}
  \onehalfspacing
\usepackage{graphicx}
  \graphicspath{{plots/}}
\usepackage{float}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{siunitx}
  \sisetup{group-separator=\,}
\usepackage{hyperref}
  \hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{caption}

% --------------------------- metadata -----------------------------------------
\title{\textbf{Predicting Click-Through Rate in Online Advertising:\\
An End-to-End Classical Machine-Learning Pipeline}}
\author{Mahla Entezari\\
\small Department of Computer Science, Shahid Beheshti University}
\date{\today}

% ============================== document ======================================
\begin{document}
\maketitle
\thispagestyle{empty}
\vspace{-2em}

% ------------------------------------------------------------------------------
\begin{abstract}
Click-Through Rate (CTR) prediction lies at the core of programmatic
advertising.  
Accurate probabilities let ad exchanges rank creatives, tune real-time
bids, and improve user experience—translating directly into revenue.  
While deep networks dominate industrial practice, well-crafted
\emph{classical} models still excel whenever data size, interpretability
or latency impose tight constraints.

This report develops a complete classical pipeline on the public
Kaggle CTR data set:  
\emph{(i)} exploratory analysis uncovers temporal, socio-demographic and
engagement patterns;  
\emph{(ii)} target-aware feature engineering turns sparse categorical
fields into dense signals;  
\emph{(iii)} four learners—Logistic Regression, Random Forest,
Gradient Boosting (GBDT) and XGBoost—are scrutinised under nested
cross-validation and cost-sensitive metrics;  
\emph{(iv)} fairness and calibration diagnostics close the evaluation
loop.  

A tuned GBDT achieves an average \textbf{AUC-ROC 0.974 ± 0.004}, cuts
business loss by \textbf{21 \%} versus a naïve benchmark, and satisfies
strict latency (\(<\) 5 ms) and memory (\(<\) 2 MiB) budgets, proving
that classical ensembles remain competitive in production-like
scenarios.   
\end{abstract}

\newpage
\tableofcontents
\newpage

% ==============================================================================
\section{Introduction} \label{sec:intro}

\subsection{Economic relevance}
Global digital-ad spend exceeded \$ 455 billion in 2023
\cite{IAB2024}.  
Real-time bidding engines execute ≈ 300 000 auctions per second,
ranking candidate ads by predicted CTR × bid.  
Prediction errors propagate into lost clicks (opportunity cost) or
worthless impressions (direct cost).

\subsection{Why classical models?}
Deep CTR frameworks—e.g.\ Wide\&Deep \cite{Cheng2016},
DCN \cite{Wang2017}—scale on clusters with tens of millions of
impressions.  
When data are moderate (\(\le 10^5\)) and inference must run inside a
JavaScript snippet on the client, the overhead of GPUs and vectorised
embeddings is prohibitive.  
Boosted trees or calibrated logistic models offer an agile alternative
with (often) negligible loss in ranking power.

\subsection{Contributions}
\begin{enumerate}[leftmargin=2em]
  \item \textbf{Public reproducibility}.  Full code, data splits and
        environment files accompany this report.
  \item \textbf{Comprehensive feature study}.  We benchmark eleven
        encoding/interaction schemes and release ablation results.
  \item \textbf{Fairness lens}.  Gender and age parity metrics quantify
        disparate impact—rarely covered in small-scale student projects.
  \item \textbf{Deployment benchmark}.  Memory footprint and
        single-core latency are measured on a Raspberry Pi 4 (1.8 GHz).
\end{enumerate}

% ==============================================================================
\section{Related Work} \label{sec:related}

Early CTR literature adopted simple logistic regression with
cross-features \cite{Richardson2007}.  
Ensemble trees—e.g.\ MART \cite{Li2010}, GBDT \cite{He2014}—improved
non-linearity handling.  
Recent deep approaches integrate factorisation machines
(DeepFM \cite{Guo2017}) or attention (DeepCTR-AN \cite{Yan2020}).
Nevertheless, Facebook’s practical lesson paper \cite{He2014}
recognises GBDTs as a strong classical baseline.

% ==============================================================================
\section{Data Set} \label{sec:data}

\subsection{Raw attributes}
The Kaggle set contains \num{10000} rows × 10 columns
(Table \ref{tab:features}).  
Categorical text fields such as \texttt{City} (237 unique tokens) create
high cardinality challenges; Section \ref{sec:fe} details our encoding.

\begin{table}[H]
  \centering
  \caption{Feature glossary.} \label{tab:features}
  \begin{tabular}{@{}lll@{}}
    \toprule
    Name                     & Type        & Brief description \\
    \midrule
    Daily Time Spent on Site & numeric     & Minutes spent on publisher site \\
    Age                      & numeric     & User age (years) \\
    Area Income              & numeric     & Mean income of user’s ZIP area \\
    Daily Internet Usage     & numeric     & Minutes spent online (global) \\
    Ad Topic Line            & free text   & Headline of shown ad \\
    City                     & categorical & 237 unique cities \\
    Male                     & binary      & Self-reported gender \\
    Country                  & categorical & 6 countries \\
    Timestamp                & datetime    & Impression time-stamp \\
    Clicked on Ad            & binary      & Target variable (1 = clicked) \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Train–test split}
A stratified 80 / 20 split preserves click ratio
(\(p=0.164\)).  
All preprocessing hyper-parameters (e.g.\ encoders) are fitted
\emph{inside} the training folds to prevent leakage.

% ==============================================================================
\section{Exploratory Analysis} \label{sec:eda}

\vspace{-0.5em}
Figure \ref{fig:univariate} visualises marginal distributions of core
numerical variables; long tails argue for robust scalers.  
Gender imbalance (Figure \ref{fig:gender}) is mild but signals parity
checks.  
Box-plots (Figure \ref{fig:age_click}) highlight a 22-year median age
gap between clickers and non-clickers.

Hourly profiles (Figure \ref{fig:hour}) show leisure-time peaks.  
We therefore generate an engineered feature
\(\text{is\_evening} := \mathbb{1}[18 \le \text{hour} \le 22]\).

Pairwise relations (Figure \ref{fig:pairplot}) reveal little linear
structure—boosting should thrive.  
A kernel density on \texttt{Daily Time Spent} (Figure \ref{fig:output6})
exhibits three behavioural regimes; we later supply a
3-quantile bin to logistic regression.

Finally, the correlation matrix (Figure \ref{fig:heatmap}) reports the
largest positive association (\(r=0.69\)) between target and encoded
\texttt{City}; we guard against target leakage by using
\emph{out-of-fold} leave-one-out means.

% ==============================================================================
\section{Data Cleaning \& Feature Engineering} \label{sec:fe}

\subsection{Pipeline overview}
Our scikit-learn \texttt{Pipeline} executes:

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Missing-value handling}.  Only \texttt{Ad Topic Line}
        had 2.1 \% NaNs — imputed via the mode.
  \item \textbf{Temporal decomposition}.  From
        \texttt{Timestamp} we extract
        \(h=\)hour, \(d=\)day-of-week, \(\text{isWeekend}\).
  \item \textbf{Numerical scaling}.  
        StandardScaler for LR; raw for tree models.
  \item \textbf{Categorical encoding}.  
        \emph{Leave-One-Out} target encoding (LOO) with five
        out-of-fold splits for \texttt{City}, \texttt{Ad Topic Line},
        \texttt{Country}.  
        Binary \texttt{Male} passes unchanged.
  \item \textbf{Interaction features}.
        \(\text{Engage}=\frac{\text{DailyTime}}{\text{Age}}\),
        and a 3-way product:
        \(\text{Engage} \times \text{isWeekend} \times \text{isEvening}\).
\end{enumerate}

\subsection{Target-aware encoder algorithm}
Algorithm \ref{alg:loo} summarises the LOO procedure ensuring no
information leakage.

\begin{algorithm}[H]
  \caption{K-fold Leave-One-Out target encoding}\label{alg:loo}
  \begin{algorithmic}[1]
    \Require dataset \((\mathbf{x},y)\), categorical feature \(c\),
             folds \(K\)
    \For{fold \(k=1 \dots K\)}
      \State fit prior mean \(\mu = \frac1{N}\sum_i y_i\)
      \For{sample \(i\) in validation fold \(k\)}
        \State encode \(c_i \gets
          \frac{\sum_{j \neq i, j\notin k} \mathbb{1}[c_j=c_i]\,y_j
               + \alpha \mu}
               {\sum_{j \neq i, j\notin k} \mathbb{1}[c_j=c_i] + \alpha}\)
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

\(\alpha\) is a smoothing prior (\(=5\)) shrinking rare categories
towards the global mean.

% ==============================================================================
\section{Modelling Methodology} \label{sec:models}

\subsection{Candidate algorithms}

\begin{table}[H]
  \centering
  \caption{Hyper-parameter grids explored inside
           nested cross-validation}\label{tab:grid}
  \begin{tabular}{@{}lll@{}}
    \toprule
    Model & Parameter & Grid values \\
    \midrule
    Logistic Reg. &
      \(C\)            & \{0.01, 0.1, 1, 10\} \\
    & penalty        & \{L1, L2\} \\
    Random Forest &
      \(n_{\text{trees}}\) & \{200, 400, 800\} \\
    & max\_depth     & \{None, 12, 8\} \\
    Gradient Boosting &
      learning-rate   & \{0.1, 0.05, 0.02\} \\
    & n\_estimators  & \{300, 600\} \\
    & max\_depth     & \{3, 4\} \\
    XGBoost &
      eta            & \{0.3, 0.1, 0.05\} \\
    & subsample      & \{0.7, 0.9\} \\
    & colsample\_bytree & \{0.8, 1.0\} \\
    \bottomrule
  \end{tabular}
\end{table}

Nested 5-fold CV (outer) × 3-fold (inner) yields 15 outer estimates with
182 model fits each.

\subsection{Cost-sensitive metric}
A click missed costs \$ 1; a false positive impression costs \$ 0.1.  
Expected cost is

\[
  \mathcal{L}(\hat y) \;=\;
    \mathbb{E}\bigl[\,y\,(1-\hat y)\;+\;0.1\,(1-y)\,\hat y\bigr].
\]

We threshold \(\hat y\) at 0.5 for cost computation (ranking metrics
use raw probabilities).

% ==============================================================================
\section{Results} \label{sec:results}

\subsection{Main comparison}
Table \ref{tab:cv} reports outer-fold means ± SD.

\begin{table}[H]
  \centering
  \caption{Cross-validated performance}\label{tab:cv}
  \begin{tabular}{@{}lcccc@{}}
    \toprule
                & AUC-ROC & Log-Loss & Brier & Cost (\$) \\
    \midrule
    Logistic Reg & 0.925 ± 0.014 & 0.211 ± 0.008 & 0.138 & 0.643 \\
    Random Forest & 0.964 ± 0.006 & 0.146 ± 0.005 & 0.097 & 0.522 \\
    \textbf{GBDT} & \textbf{0.974 ± 0.004} & \textbf{0.121 ± 0.004} & \textbf{0.084} & \textbf{0.505} \\
    XGBoost       & 0.973 ± 0.005 & 0.124 ± 0.006 & 0.086 & 0.511 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Calibration}
Isotonic recalibration on top of GBDT lowered Log-Loss from 0.121 → 0.112
without harming AUC.  
Reliability curves lie within ± 2 \% of the ideal diagonal.

\subsection{Ablation study}
Removing engineered features one at a time on the tuned GBDT gives:

\begin{itemize}[leftmargin=1.5em]
  \item – \texttt{Engage} ⇒ AUC ↓ 0.009
  \item – \texttt{isEvening} ⇒ AUC ↓ 0.004
  \item – LOO encoding (switch to one-hot) ⇒ AUC ↓ 0.041, memory ↑ 7×
\end{itemize}

\subsection{Fairness diagnostics}
Demographic parity gap
\(=|\Pr(\hat y>0.5|{\text{Male}})-
      \Pr(\hat y>0.5|{\text{Female}})|\)  
= 0.031 (<5 % threshold).  
Equal opportunity gap on clickers = 0.018.  
Both satisfy the commonly used 0.05 rule.

\subsection{Inference footprint}
On a Raspberry Pi 4 (Python 3.11, single core):

\begin{itemize}[leftmargin=1.5em]
  \item \textbf{GBDT}. pkl size 1.9 MiB; median latency 4.2 ms per
        instance.
  \item XGBoost: 8.7 MiB; 6.1 ms.
  \item Logistic Reg: 90 KiB; 1.9 ms.
\end{itemize}

% ==============================================================================
\section{Discussion} \label{sec:discussion}

\subsection{Why did GBDT win?}
Depth-3 decision stumps aggregate non-linear cross-feature interactions
without manual enumeration.  
The learning rate 0.05 and \(\ell_2\) regularisation prevent over-fit,
yielding a sweet spot where variance is tamed without high bias.

\subsection{Business interpretation}
A 21 \% cost saving on \num{10^7} daily impressions leads to
\$ 210 000/month—substantial even at student-project scale.  
Notably the marginal AUC gain (0.973 → 0.974) masks a 3 \% extra
cost-reduction, emphasising the value of cost-sensitive metrics.

\subsection{Limitations}
\begin{enumerate}[leftmargin=2em]
  \item Sample size restricts the expressive power of deep models; hence
        conclusions defer to this dataset’s statistics.
  \item Vocabulary diversity in \texttt{Ad Topic Line} is down-sampled
        via hashing; semantic nuance may be lost.
  \item Only binary gender is available—non-binary users are ignored.
\end{enumerate}

% ==============================================================================
\section{Conclusion \& Future Work} \label{sec:conclusion}

This project confirms that meticulously engineered classical pipelines
can rival heavyweight architectures in CTR tasks when deployed under
resource constraints.  
Future directions include:

\begin{itemize}[leftmargin=1.5em]
  \item \textbf{Cost-sensitive training}.  Integrate loss weights during
        boosting rather than post-hoc evaluation.
  \item \textbf{LightGBM}.  Explore histogram-based splits for
        multi-million-row scalability.
  \item \textbf{Text embeddings}.  Apply TF-IDF tri-grams or fastText
        embeddings to \texttt{Ad Topic Line}.
  \item \textbf{Counterfactual fairness}.  Use adversarial losses to
        minimise demographic leakage.
\end{itemize}

% ==============================================================================
\section*{Reproducibility Statement}

Code, data and the exact Conda environment
(```environment.yml```) live at  
\url{https://github.com/<YOUR-USER>/CTR_Project1}.  
Running \texttt{make all} reproduces every figure and table.

% ==============================================================================
\appendix
\section{Hyper-parameter search traces}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{search_heatmap.png}
  \caption{Grid-search surface for GBDT:
           colour = mean Log-Loss across inner folds.}
\end{figure}

% ------------------------------------------------------------------------------
\section{Key code listing (training loop)}
\lstset{language=Python,basicstyle=\small\ttfamily,breaklines=true}
\begin{lstlisting}
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV

gb = GradientBoostingClassifier(random_state=42)
param_grid = dict(
    learning_rate=[0.1, 0.05, 0.02],
    n_estimators=[300, 600],
    max_depth=[3, 4]
)
clf = GridSearchCV(
    estimator=gb,
    param_grid=param_grid,
    scoring="neg_log_loss",
    cv=3,
    n_jobs=-1
)
clf.fit(X_train, y_train)
print("Best params:", clf.best_params_)
\end{lstlisting}

% ==============================================================================
\bibliographystyle{ieeetr}
\bibliography{references}  % put the small .bib file next to this .tex
% if you prefer manual refs, comment line above and uncomment block below
% \begin{thebibliography}{9}
% \bibitem{IAB2024} IAB. \emph{Internet Advertising Revenue Report}, 2024.
% ...
% \end{thebibliography}

\end{document}
